{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](https://user-images.githubusercontent.com/5395649/46774810-22395980-ccb9-11e8-8f1a-535769d657ec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Command Recognition Task with Keras \n",
    "\n",
    "In this notebook, we will train a simple convolutional neural network (CNN) to recognize utterances of different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "import hashlib \n",
    "import re\n",
    "import pandas as pd \n",
    "import IPython.display as ipd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "from tqdm import tqdm \n",
    "import pickle as pkl \n",
    "\n",
    "#from preprocess import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the different library versions we will be using in this notebook.\n",
    "\n",
    "[librosa](https://librosa.github.io/librosa/) is an interesting python library for audio analysis. The paper describing the library can be found [here](http://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0 2.0.8 0.6.2 0.20.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__, keras.__version__, librosa.__version__, pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we get started... \n",
    "\n",
    "* **If you are unfamiliar with signal processing in general, we recommend you review the notebook `1-intro-to-audio-data.ipynb`. To run this notebook, you'll need to download the raw data from Oracle Cloud Infrastructure Object Storage. Execute the shell script `get-data-from-oci.sh` and change the directory where you want the data to be stored on your drive. **\n",
    "\n",
    "\n",
    "* You don't need the raw data to run this notebook. You will be able to load a pre-processed dataset that is stored in the git repo. This dataset will work for all default parameters provided in this notebook. \n",
    "\n",
    "Let us define some variables that will be used throughout the notebook. \n",
    "\n",
    "* `data_dir` points to the `wav` folder of the dataset. On the platform, this means `/home/datascience/data/`.\n",
    "* `MAX_NUM_WAVS_PER_CLASS` is the maximum number of audio clip examples per word class. Leave it as is for now.\n",
    "\n",
    "## A few words on the dataset \n",
    "\n",
    "The dataset we use is the [Speech Commands Dataset](https://arxiv.org/abs/1804.03209). This dataset includes 105k utterances of 35 words. Each word has its corresponding folder containing between 1k and 4k examples. In total, **over 2K different speakers were recorded**. Each clip lasts 1 second at a sampling rate of 16 kHz. The uncompressed data take about 4Gb on disk. \n",
    "\n",
    "The dataset is licensed under the [Creative Commons BY 4.0 license](https://creativecommons.org/licenses/by/4.0/). See the LICENSE file in the dataset folder for full details. Its original location was at\n",
    "[http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the root directory of the dataset. If you are running this notebook on the Platform, this\n",
    "# folder should be /home/datascience/data/\n",
    "data_dir = '/home/datascience/data/'\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "# Training, validation and testing data split. An 80-10-10% split is standard. \n",
    "training_fraction = 80.0\n",
    "validation_fraction = 10.0\n",
    "testing_fraction = 10.0 \n",
    "\n",
    "# If you have not downloaded the original dataset and you want to \n",
    "# use the pre-processed, transformed dataset, set the flag \"list_available_words\" to False\n",
    "list_available_words = False \n",
    "\n",
    "if list_available_words: \n",
    "    # Lets find what words are available for us to classify: \n",
    "    words_available = [ d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d)) ] \n",
    "\n",
    "    # This is the list of available utterances one can use in their classifier\n",
    "    print(words_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular model, **we will train a classifier whose purpose will be to \n",
    "classify utterances of the words \"right\", \"eight\", \"cat\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_selected = ['right', 'eight', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function I took directly from the `README` file that came with the dataset. This is a very useful function that ensures testing/validation data does not make its way in the training dataset. We will be using it to assign the sample (training, validation, testing) to which each audio clip belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Warden (2018) : https://arxiv.org/abs/1804.03209\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted \n",
    "    for example. To keep this stability, a hash of the filename is taken and used \n",
    "    to determine which set it should belong to. This determination only depends on \n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "    \n",
    "    It's also useful to associate particular files as related (for example words \n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is \n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and \n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "    \n",
    "    Args\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "    \n",
    "    Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    #print(base_name)\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode('utf-8')\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it. \n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest() \n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                     (100.0 / MAX_NUM_WAVS_PER_CLASS)) \n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the data transformation that we apply to each audio clip before feeding the clip to the CNN model. Take a look at the [intro notebook](1-intro-to-audio-data.ipynb) where we discuss spectrogram and MFCCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_processing(filepath, max_len=32):\n",
    "    \"\"\" Compute MFCCs for a given clip\n",
    "    \n",
    "    Args: \n",
    "        - filepath (str) : path of the wav file to analyze. \n",
    "        - max_len (int) : \n",
    "        \n",
    "    Returns: \n",
    "        - Mel-frequency cepstral coefficients array. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we are loading the audio clip using librosa.\n",
    "    waveform, sampling_rate = librosa.load(filepath, mono=True, sr=None)\n",
    "    # compute the Mel-frequency cepstral coefficients. n_mfcc \n",
    "    # represents the number of coefficients to return. \n",
    "    mfcc = librosa.feature.mfcc(waveform, sr=16000, n_mfcc=20)\n",
    "    \n",
    "    # Not all clips have the same duration. Padding along the time dimension to \n",
    "    # ensure each clip has the same dimensions. Each image fed to the CNN needs \n",
    "    # to have the same dimensions. \n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0),(0, max_len-mfcc.shape[1])), mode='constant')\n",
    "    return mfcc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training, validation and testing sets\n",
    "\n",
    "In the cells below, we build the training, validation, and training datasets that will be used to train the CNN model. \n",
    "We recommend that you load the pre-processed data. It takes a while to compute the MFCCs for these sound clips. **The pre-processed, feature-extracted data was generated for words (`right`, `eight`, `cat`) only.** If you want to use different words, you need to download the raw, original dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 2, 'right': 0, 'eight': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping between index for one-hot encoding and the class label:\n",
    "class_to_int = {label:i for i, label in enumerate(words_selected)}\n",
    "print(class_to_int)\n",
    "\n",
    "# Save that lookup dictionary to disk: \n",
    "pkl.dump(class_to_int, open('./class_label_lookup.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Either load the processed and feature-extracted dataset or generate your own. \n",
    "# Set this value to True unless you have downloaded the raw data locally using the shell script. \n",
    "\n",
    "load_preprocessed_data = True\n",
    "\n",
    "if load_preprocessed_data: \n",
    "    df = pkl.load(open('../data/processed_data.pkl','rb'))\n",
    "else: \n",
    "    tqdm.pandas()\n",
    "    df = pd.DataFrame(columns=['filename','label','sample'])\n",
    "\n",
    "    for w in words_selected: \n",
    "        files = [ f for f in os.listdir(data_dir + w) if f[-4:] == '.wav' ]\n",
    "        labels = [ w for f in files]\n",
    "        sample = [ which_set(f, validation_fraction, testing_fraction) for f in files ]\n",
    "        tmp = pd.DataFrame({'filename':files, 'label':labels, 'sample':sample})\n",
    "        df = df.append(tmp, ignore_index=True)\n",
    "   \n",
    "    # adding the full path, mfcc data and class label mapping \n",
    "    df['full_path'] = data_dir + df['label'] + \"/\" + df['filename']\n",
    "    df['mfcc'] = df['full_path'].progress_apply(lambda x: data_processing(x))\n",
    "    df['y_hot'] = df['label'].map(class_to_int)\n",
    "    \n",
    "    # save data to disk: \n",
    "    df.to_pickle('../data/processed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>sample</th>\n",
       "      <th>full_path</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>y_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b88bf70_nohash_0.wav</td>\n",
       "      <td>right</td>\n",
       "      <td>training</td>\n",
       "      <td>/home/datascience/data/right/1b88bf70_nohash_0...</td>\n",
       "      <td>[[-433.3795378509684, -434.2466677165984, -436...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b12bef84_nohash_1.wav</td>\n",
       "      <td>right</td>\n",
       "      <td>training</td>\n",
       "      <td>/home/datascience/data/right/b12bef84_nohash_1...</td>\n",
       "      <td>[[-395.73158891078725, -374.81060233119285, -3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05b2db80_nohash_1.wav</td>\n",
       "      <td>right</td>\n",
       "      <td>training</td>\n",
       "      <td>/home/datascience/data/right/05b2db80_nohash_1...</td>\n",
       "      <td>[[-265.1568901743038, -266.41782567801494, -27...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b66f4f93_nohash_5.wav</td>\n",
       "      <td>right</td>\n",
       "      <td>training</td>\n",
       "      <td>/home/datascience/data/right/b66f4f93_nohash_5...</td>\n",
       "      <td>[[-549.6284243645459, -551.6706347878757, -558...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750e3e75_nohash_0.wav</td>\n",
       "      <td>right</td>\n",
       "      <td>validation</td>\n",
       "      <td>/home/datascience/data/right/750e3e75_nohash_0...</td>\n",
       "      <td>[[-513.6241339519378, -511.58181167935845, -51...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  label      sample  \\\n",
       "0  1b88bf70_nohash_0.wav  right    training   \n",
       "1  b12bef84_nohash_1.wav  right    training   \n",
       "2  05b2db80_nohash_1.wav  right    training   \n",
       "3  b66f4f93_nohash_5.wav  right    training   \n",
       "4  750e3e75_nohash_0.wav  right  validation   \n",
       "\n",
       "                                           full_path  \\\n",
       "0  /home/datascience/data/right/1b88bf70_nohash_0...   \n",
       "1  /home/datascience/data/right/b12bef84_nohash_1...   \n",
       "2  /home/datascience/data/right/05b2db80_nohash_1...   \n",
       "3  /home/datascience/data/right/b66f4f93_nohash_5...   \n",
       "4  /home/datascience/data/right/750e3e75_nohash_0...   \n",
       "\n",
       "                                                mfcc  y_hot  \n",
       "0  [[-433.3795378509684, -434.2466677165984, -436...      0  \n",
       "1  [[-395.73158891078725, -374.81060233119285, -3...      0  \n",
       "2  [[-265.1568901743038, -266.41782567801494, -27...      0  \n",
       "3  [[-549.6284243645459, -551.6706347878757, -558...      0  \n",
       "4  [[-513.6241339519378, -511.58181167935845, -51...      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training      7709\n",
       "testing        998\n",
       "validation     889\n",
       "Name: sample, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: counts for each sample category. \n",
    "df['sample'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and testing sets\n",
    "\n",
    "Below we split the datasets by training, validation and evaluation (test) samples. \n",
    "We will use the mfcc values as the covariates/features for each clip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset: \n",
    "X_train = df[df['sample'] == \"training\"]['mfcc'].values\n",
    "Y_train = df[df['sample'] == \"training\"]['y_hot'].values\n",
    "\n",
    "# Validation dataset: \n",
    "X_valid = df[df['sample'] == \"validation\"]['mfcc'].values\n",
    "Y_valid = df[df['sample'] == \"validation\"]['y_hot'].values\n",
    "\n",
    "# Evaluation dataset: \n",
    "X_test = df[df['sample'] == \"testing\"]['mfcc'].values\n",
    "Y_test = df[df['sample'] == \"testing\"]['y_hot'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping the MFCCs  \n",
    "Xtrain = np.asarray([ sub.reshape(20,32,1) for sub in X_train ])\n",
    "Xvalid = np.asarray([ sub.reshape(20,32,1) for sub in X_valid ])\n",
    "Xtest = np.asarray([ sub.reshape(20,32,1) for sub in X_test ])\n",
    "\n",
    "# Converting the index values to categorical values using the Keras to_categorical() function \n",
    "Ytrain = to_categorical(Y_train)\n",
    "Yvalid = to_categorical(Y_valid)\n",
    "Ytest = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7709, 20, 32, 1), (7709, 3), (998, 20, 32, 1), (998, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make sure we have the right dimensions for all these arrays: \n",
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are now ready to train the Convolutional Neural Network (CNN) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN model \n",
    "\n",
    "I adopted a slight variation of the [Sainath & Parada (2015)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43969.pdf) `cnn-trad-fpool3` model for the purpose of this demo. I invite the reader to read this paper for more details. \n",
    "\n",
    "This is a simple model that performs well. I adopted convolution kernels of equal size in both frequency and time space. I removed the linear low-rank layer. I also used fewer filters and the kernel size is smaller. \n",
    "\n",
    "I invite the reader to modify the network architecture and try different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (CNN) Model Definition:  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolution layer (4x8), 32 filters \n",
    "model.add(Conv2D(32, kernel_size=(4, 8), activation='relu', input_shape=(20, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Convolution layer (2x4), 16 filters \n",
    "model.add(Conv2D(16, kernel_size=(2, 4), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# DNN FC layer: \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# softmax layer (3 words)\n",
    "model.add(Dense(len(words_selected), activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model. \n",
    "\n",
    "Ten epochs is good enough to start. It will give you an accuracy superior to 0.90. I invite the reader to train the models for 100+ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7709 samples, validate on 889 samples\n",
      "Epoch 1/10\n",
      "7709/7709 [==============================] - 6s - loss: 3.1944 - acc: 0.4875 - val_loss: 0.8083 - val_acc: 0.6130\n",
      "Epoch 2/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.9695 - acc: 0.5763 - val_loss: 0.7632 - val_acc: 0.6794\n",
      "Epoch 3/10\n",
      "7709/7709 [==============================] - 5s - loss: 0.8137 - acc: 0.6466 - val_loss: 0.6450 - val_acc: 0.7638\n",
      "Epoch 4/10\n",
      "7709/7709 [==============================] - 5s - loss: 0.6857 - acc: 0.7140 - val_loss: 0.4378 - val_acc: 0.8538\n",
      "Epoch 5/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.5834 - acc: 0.7579 - val_loss: 0.4211 - val_acc: 0.8605\n",
      "Epoch 6/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.5350 - acc: 0.7943 - val_loss: 0.3540 - val_acc: 0.8920\n",
      "Epoch 7/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.4810 - acc: 0.8080 - val_loss: 0.3365 - val_acc: 0.9044\n",
      "Epoch 8/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.4482 - acc: 0.8202 - val_loss: 0.3301 - val_acc: 0.8965\n",
      "Epoch 9/10\n",
      "7709/7709 [==============================] - 5s - loss: 0.4314 - acc: 0.8366 - val_loss: 0.2809 - val_acc: 0.9213\n",
      "Epoch 10/10\n",
      "7709/7709 [==============================] - 6s - loss: 0.3927 - acc: 0.8502 - val_loss: 0.2469 - val_acc: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5043ad0240>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model: \n",
    "\n",
    "model.fit(Xtrain, Ytrain, \n",
    "          batch_size=100, \n",
    "          epochs=10, \n",
    "          verbose=1, \n",
    "          shuffle=True,\n",
    "          validation_data=(Xvalid, Yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/998 [======================>.......] - ETA: 0sloss = 0.27580045937058445, accuracy = 0.9078156311430768\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model: \n",
    "\n",
    "loss, accuracy = model.evaluate(x=Xtest, y=Ytest)\n",
    "print(\"loss = {}, accuracy = {}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/998 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.text.Text at 0x7f503c485be0>,\n",
       " <matplotlib.text.Text at 0x7f503c4edcf8>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f503c4d39b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute and plot Confusion matrix \n",
    "\n",
    "Ypred = model.predict_classes(Xtest)\n",
    "cm = confusion_matrix(Y_test, Ypred, labels=[0,1,2])#, labels = words_selected)\n",
    "\n",
    "ax = seaborn.heatmap(cm, annot=True, fmt='d', \n",
    "                     linewidths=.2, \n",
    "                     xticklabels=words_selected, \n",
    "                     yticklabels=words_selected)\n",
    "\n",
    "ax.set(xlabel='predicted',ylabel='observed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./model1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Next?\n",
    "\n",
    "You can experiment and expand on the model trained in this notebook. Here are a few areas to explore: \n",
    "\n",
    "* increase the number of classes. Try to classify 10+ words \n",
    "* add noise to your examples. Create a more robust classifier. \n",
    "* try different CNN architectures \n",
    "* include an \"other\" class\n",
    "* include a \"silence\" class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "* Warden, P. 2018, \"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\", [arXiv:1804.03209](https://arxiv.org/abs/1804.03209)\n",
    "* Sainath, T.N., Parada, C. 2015 [\"Convolutional Neural Networks for Small-footprint Keyword Spotting\"]( https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43969.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the network architecture for reference. \n",
    "\n",
    "![network-architecture](https://user-images.githubusercontent.com/5395649/46776515-cb378280-ccc0-11e8-8dcb-84db4156e981.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
